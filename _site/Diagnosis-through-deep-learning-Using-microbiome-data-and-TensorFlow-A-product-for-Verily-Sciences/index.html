<!doctype html>
<!--[if lt IE 7]><html class="no-js lt-ie9 lt-ie8 lt-ie7" lang="en"> <![endif]-->
<!--[if (IE 7)&!(IEMobile)]><html class="no-js lt-ie9 lt-ie8" lang="en"><![endif]-->
<!--[if (IE 8)&!(IEMobile)]><html class="no-js lt-ie9" lang="en"><![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en"><!--<![endif]-->
<head>
<meta charset="utf-8">
<title>Diagnosis through deep learning&#58; Using microbiome data and TensorFlow &#8211; Ali A. Faruqi</title>
<meta name="description" content="Deep-learning based microbiome diagnostics">
<meta name="keywords" content="microbiome, biomedical research, diagnostics, deep learning">



<!-- Open Graph -->
<meta property="og:locale" content="en_US">
<meta property="og:type" content="article">
<meta property="og:title" content="Diagnosis through deep learning&#58; Using microbiome data and TensorFlow">
<meta property="og:description" content="Deep-learning based microbiome diagnostics">
<meta property="og:url" content="/Diagnosis-through-deep-learning-Using-microbiome-data-and-TensorFlow-A-product-for-Verily-Sciences/">
<meta property="og:site_name" content="Ali A. Faruqi">





<link rel="canonical" href="/Diagnosis-through-deep-learning-Using-microbiome-data-and-TensorFlow-A-product-for-Verily-Sciences/">
<link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Ali A. Faruqi Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<!-- Webfonts -->
<link href="//fonts.googleapis.com/css?family=Lato:300,400,700,300italic,400italic" rel="stylesheet" type="text/css">

<meta http-equiv="cleartype" content="on">

<!-- Load Modernizr -->
<script src="/assets/js/vendor/modernizr-2.6.2.custom.min.js"></script>

<!-- Icons -->
<!-- 16x16 -->
<link rel="shortcut icon" href="/favicon.ico">
<!-- 32x32 -->
<link rel="shortcut icon" href="/favicon.png">
<!-- 57x57 (precomposed) for iPhone 3GS, pre-2011 iPod Touch and older Android devices -->
<link rel="apple-touch-icon-precomposed" href="/images/apple-touch-icon-precomposed.png">
<!-- 72x72 (precomposed) for 1st generation iPad, iPad 2 and iPad mini -->
<link rel="apple-touch-icon-precomposed" sizes="72x72" href="/images/apple-touch-icon-72x72-precomposed.png">
<!-- 114x114 (precomposed) for iPhone 4, 4S, 5 and post-2011 iPod Touch -->
<link rel="apple-touch-icon-precomposed" sizes="114x114" href="/images/apple-touch-icon-114x114-precomposed.png">
<!-- 144x144 (precomposed) for iPad 3rd and 4th generation -->
<link rel="apple-touch-icon-precomposed" sizes="144x144" href="/images/apple-touch-icon-144x144-precomposed.png">



</head>

<body id="post" class="feature">

<!--[if lt IE 9]><div class="upgrade"><strong><a href="http://whatbrowser.org/">Your browser is quite old!</strong> Why not upgrade to a different browser to better enjoy this site?</a></div><![endif]-->
<nav id="dl-menu" class="dl-menuwrapper" role="navigation">
	<button class="dl-trigger">Open Menu</button>
	<ul class="dl-menu">
		<li><a href="/">Home</a></li>
		<li>
			<a href="#">About</a>
			<ul class="dl-submenu">
				<li>
					<img src="/images/avatar.jpg" alt="Ali A. Faruqi photo" class="author-photo">
					<h4>Ali A. Faruqi</h4>
					<p>Bioinformatician interested in product development and product management</p>
				</li>
				<li><a href="/about/"><span class="btn btn-inverse">Learn More</span></a></li>
				<li>
					<a href="mailto:alifar76@gmail.com"><i class="fa fa-fw fa-envelope"></i> Email</a>
				</li>
				
				
				
				
				
				
				
				
				
			</ul><!-- /.dl-submenu -->
		</li>
		<li>
			<a href="#">Posts</a>
			<ul class="dl-submenu">
				<li><a href="/posts/">All Posts</a></li>
				<li><a href="/tags/">All Tags</a></li>
			</ul>
		</li>
		
	</ul><!-- /.dl-menu -->
</nav><!-- /.dl-menuwrapper -->



<div class="entry-header">
  <div class="image-credit">Image source: <a href="http://www.dargadgetz.com/ios-7-abstract-wallpaper-pack-for-iphone-5-and-ipod-touch-retina/">dargadgetz</a></div><!-- /.image-credit -->
  <div class="entry-image">
    <img src="/images/abstract-3.jpg" alt="Diagnosis through deep learning&#58; Using microbiome data and TensorFlow">
  </div><!-- /.entry-image -->
</div><!-- /.entry-header -->


<div id="main" role="main">
  <article class="hentry">
    <header class="header-title">
      <div class="header-title-wrap">
        
          <h1 class="entry-title"><a href="/Diagnosis-through-deep-learning-Using-microbiome-data-and-TensorFlow-A-product-for-Verily-Sciences/" rel="bookmark" title="Diagnosis through deep learning&#58; Using microbiome data and TensorFlow">Diagnosis through deep learning&#58; Using microbiome data and TensorFlow</a></h1>
        
        <h2><span class="entry-date date published"><time datetime="2016-06-27T00:00:00-07:00">June 27, 2016</time></span></h2>
        
      </div><!-- /.header-title-wrap -->
    </header>
    <div class="entry-content">
      <p>I recently participated in the Kaggle-hosted data science competition <a href="https://www.kaggle.com/c/how-much-did-it-rain-ii"><em>How Much Did It Rain II</em></a> where the goal was to predict a set of hourly rainfall levels from sequences of weather radar measurements. I came in <em>first</em>! I describe my approach in this blog post.</p>

<p class="image-right"><img src="/images/front_page.png" alt="How much did it rain II" /></p>

<p>My research lab supervisor Dr John Pinney and I were in the midst of developing a set of deep learning tools for our current research programme when this competition came along. Due to some overlaps in the statistical tools and data sets (neural networks and variable-length sequences, in particular) I saw it as a good opportunity to validate some of our ideas in a different context (at least that is my <em>post hoc</em> justification for the time spent on this competition!). In the near future I will post about the research project and how it relates to this problem.</p>

<p>One of things which inspired me to write this up, other than doing well in the competition, was the awesome <a href="http://benanne.github.io/2015/03/17/plankton.html">blog post</a> by Sander Dieleman where he describes his team’s winning approach in another Kaggle competition I took part in. So following the outline of that article, here’s my solution to this problem.</p>

<h2 id="introduction">Introduction</h2>

<h3 id="the-problem">The problem</h3>

<p>The goal of the competition was to use sequences of polarmetric weather radar data to predict a set of hourly rain gauge measurements recorded over several months in 2014 over the US midwestern corn-growing states. These radar readings represent instantaneous precipitation rates and is used to estimate rainfall levels over a wider area (e.g. nationwide) than can practically be covered by rain gauges.</p>

<p>For the contest, each radar measurement was condensed into 22 features. These include the minutes past the top of the hour that the radar observation was carried out, the distance of the rain gauge from the radar, and various reflectivity and differential phase readings of both the vertical column above and the areas surrounding the rain gauge. Up to 19 radar records are given per hourly rain gauge reading (and as few as a single record); interestingly the number of radar measurements provided should itself contain some information on the rainfall levels as it is apparently not uncommon for meteorologists to request multiple radar sweeps when there are fast-moving storms.</p>

<p>The preditions were evaluated based on the mean absolute error (MAE) relative to actual rain gauge readings.</p>

<h3 id="the-solution-recurrent-neural-networks">The solution: Recurrent Neural Networks</h3>
<p>The prediction of cumulative values from variable-length sequences of vectors with a ‘time’ component is highly reminiscent of the so-called <em>Adding Problem</em> in machine learning—a toy sequence regression task that is designed to demonstrate the power of recurrent neural networks (RNN) in learning long-term dependencies (see <a href="http://arxiv.org/abs/1504.00941">Le et al.</a>, Sec. 4.1, for a recent example):</p>

<figure>
<center>
<img src="/images/RNN_adding.png" alt="The Adding Problem" width="475" />
</center>
<figcaption>
The prediction target of 1.7 is obtained by adding up the numbers in the top row where the corresponding number in the bottom row is equal to one (i.e. the green boxes). The regression task is to infer this generative model from a training set of random sequences of arbitrary lengths and their targets.
</figcaption>
</figure>

<p>In our rainfall prediction problem, the situation is somewhat less trivial as there is still the additional step of inferring the rainfall ‘numbers’ (the top row) from radar measurements. Furthermore, instead of binary 0/1 values (bottom row) one has continuous time readings between 0 and 60 minutes that have somewhat different roles. Nevertheless, the underlying structural similarities are compelling enough to suggest that RNNs are well-suited for the problem.</p>

<p>In the <a href="https://www.kaggle.com/c/how-much-did-it-rain">previous version</a> of this contest (which I did not participate in), gradient boosting was the undisputed star of the show; neural networks, to my knowledge, were not deployed with much success. If RNNs would prove to be as unreasonably effective here as they are in an increasing array of problems in machine learning, then I might have a chance of coming up with a unique and good solution.</p>

<p>For a overview of RNNs, the <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">blog post</a> by Andrej Karpathy is as good a general introduction to the subject as you will find anywhere.</p>

<h3 id="software-and-hardware">Software and hardware</h3>
<p>I used <a href="https://www.python.org/">Python</a> with <a href="http://deeplearning.net/software/theano/">Theano</a> throughout and relied heavily on the <a href="http://lasagne.readthedocs.org/en/latest/index.html">Lasagne</a> layer classes to build the RNN architectures. Additionally, I used <a href="http://scikit-learn.org/stable/">scikit-learn</a> to implement the cross-validation splits, and <a href="http://pandas.pydata.org/">pandas</a> and <a href="http://www.numpy.org/">NumPy</a> to process and format the data and submission files. To this day I have an irrational aversion to <a href="http://matplotlib.org/">Matplotlib</a> (even with <a href="http://stanford.edu/~mwaskom/software/seaborn/">Seaborn</a>) and so all my plots were done in <a href="https://www.r-project.org/">R</a>.</p>

<p>I trained the models on several NVIDIA GPUs in my lab, which include two Tesla K20 and three M2090 cards.</p>

<h2 id="the-data">The data</h2>

<h3 id="pre-processing">Pre-processing</h3>
<p>From the outset, there were several challenges brought about by certain features of the data:</p>

<ol>
  <li>
    <p>Extreme and unpredictable outliers</p>
  </li>
  <li>
    <p>Variable sequence lengths and irregular radar measurement times</p>
  </li>
  <li>
    <p>A training set with non-independently distributed samples</p>
  </li>
</ol>

<h4 id="extreme-and-unpredictable-outliers">Extreme and unpredictable outliers</h4>
<p>It was well-documented from the start, and much discussed in the competition forum, that a large proportion of the hourly rain gauge measurements were not to be trusted (e.g. clogged rain gauges). Given that some of these values are several orders of magnitude higher than what is physically possible anywhere on earth, the MAE values participants were reporting were dominated by these extreme outliers. However, since the evaluation metric was the MAE rather than the root-mean-square error (RMSE), one can simply view the outliers as an annoying source of extrinsic noise in the evaluation scores; the absolute values of the MAE are however, in my view, close to meaningless without prior knowledge of typical weather patterns in the US midwest.</p>

<p>The approach I and many others took was simply to exclude from the training set the rain gauges with readings above 70mm. Over the course of the competition I experimented with several different thresholds from 53mm to 73mm, and did a few runs where I removed this pre-processing step altogether. Contrary to what was reported in the previous version of this competition, this had very little effect on the performance of the model (positive or negative); it appears, and I speculate, that the RNN models had learnt to ignore the outliers, as suggested by the very reasonable maximum values of expected hourly rain gauge levels predicted for the test set (~45-55mm).</p>

<h4 id="variable-sequence-lengths-and-irregular-radar-measurement-times">Variable sequence lengths and irregular radar measurement times</h4>
<p>The weather radar sequences varied in length from one to 19 readings per hourly rain gauge record. Furthermore these readings were taken at seemingly random points within the hour. In other words, this was not your typical time-series dataset (EEG waveforms, daily stock market prices, etc).</p>

<p>One attractive feature of RNNs is that they accept input sequences of varying lengths due to weight sharing in the hidden layers. Because of this, I did not do any pre-processing beyond removing the outliers (as described above) and replacing any missing radar feature values with zero; I retained each timestamp as a component in the feature vector and preserved the sequential nature of the input. The idea was to implement an end-to-end learning framework and, for this reason, with not a small touch of laziness thrown in, I did not bother to implement any feature engineering.</p>

<h4 id="a-training-set-with-non-independently-distributed-samples">A training set with non-independently distributed samples</h4>
<p>The training set consists of data from the first 20 days of each month and the test set data from the remaining days. This ensures that both sets are more or less independent. However, as was pointed out in the competition forum, because the calendar time and location information is omitted, it is impossible to construct a local validation holdout subset that is truly independent from the rest of the training set; specifically, there is no way of ensuring that any two gauge readings are not correlated in time or space. This had implications in that it was very difficult to detect cases of overfitting without submissions to the public leaderboard (see Training section below for more details).</p>

<h3 id="data-augmentation">Data augmentation</h3>
<p>One common method to reduce overfitting is to augment the training set via label-preserving transformations on the data. The classic examples in image classification tasks include cropping and shifting the images, and in many cases rotating, perturbing the brightness and colour of the images and <a href="http://googleresearch.blogspot.co.uk/2015/07/how-google-translate-squeezes-deep.html">introducing noise</a>.</p>

<p>For the radar sequence data, I implemented a form of <em>Dropin</em> augmentation on the data where the sequences were lengthened to a fixed length in the time dimension by duplicating the vectors at random time points. This is loosely speaking the opposite of performing <a href="http://arxiv.org/abs/1207.0580">dropout</a> on the input layer, hence the name. This is illustrated in the figure below:</p>

<figure>
<center>
<img src="/images/RNN_01.png" alt="Dropin augmentation" width="475" />
</center>
<figcaption>
Dropin augmentations of a length-5 sequence to length-8 sequences. The number labels are the timestamps of the given data points (minutes past the hour). Note that the temporal order of the augmented sequence is preserved.
</figcaption>
</figure>

<p>Over the competition I experimented with fixed sequence lengths of 19, 21, 24 and 32 timepoints. I found that stretching out the sequence lengths beyond 21 timesteps was too aggressive as the models began to underfit.</p>

<p>My original intention was to find a way to standardise the sequence lengths to facilitate mini-batch stochastic gradient descent training. However it soon became clear that it could be a useful way to force the network to learn to factor in the time <em>intervals</em> between observations; specifically, this is achieved by encouraging the network to ignore readings when the intervals are zero. To the best of my knowledge this is a novel, albeit simple, idea.</p>

<h2 id="rnn-architecture">RNN architecture</h2>
<p>The best performing architecture I found over the competition is a 5-layer deep stacked bidirectional (vanilla) RNN with 64-256 hidden units, with additional single dense layers after each hidden stack. At the top of the network the vector at each time position is fed into a dense layer with a single output and a <a href="http://machinelearning.wustl.edu/mlpapers/paper_files/icml2010_NairH10.pdf">Rectified Linear Unit (ReLU)</a> non-linearity. The final result is obtained by taking the mean of the predictions from the entire top layer. I will explain the evolution of this model using figures in the following section. This roughly mirrors the way I developed the models over the course of the competition.</p>

<h3 id="design-evolution">Design evolution</h3>
<p>The basic model inspired by the <em>adding problem</em> is a single layer RNN:</p>

<figure>
<center>
<img src="/images/RNN_arc_1.png" alt="RNN-basic" width="400" />
</center>
<figcaption>
Basic many-to-one RNN.
</figcaption>
</figure>

<p>The RNN basically functions as an integration machine and is employed in a many-to-one fashion.</p>

<p>The law of gravity aside, and not to mention the second law of thermodynamics, there is nothing preventing us from viewing the problem as rain flying up from rain gauges on the ground and reconstituting itself as clouds. Hence we can introduce a reverse direction and consider a bidirectional RNN:</p>

<figure>
<center>
<img src="/images/RNN_arc_2.png" alt="RNN-bidirectional" width="400" />
</center>
<figcaption>
Bidirectional, many-to-one, RNN. The final output is the mean of the two, one unit wide, dense layers.
</figcaption>
</figure>

<p>A separate class of architectures imagines predictors situated across the time dimension at the top of the network, each with a unique view to the past and the future. In this scenario we pool together the outputs from the entire hidden layer to obtain a consensus prediction:</p>

<figure>
<center>
<img src="/images/RNN_arc_3.png" alt="RNN-consensus" width="450" />
</center>
<figcaption>
Pooling the predictions of the outputs from all the hidden layer timepoints.
</figcaption>
</figure>

<p>Finally, there are all manner of enhancements one can employ to create a deep network such as stacking and inserting dense layers between stacks and at the top of the network (see <a href="http://arxiv.org/abs/1312.6026">Pascanu et al.</a>). In addition to these I included a linear layer to reduce the dimension of the feature vectors from 22 to 16, in part to guard against overfitting and partly because the models were beginning to take too long to train (see below):</p>

<figure>
<center>
<img src="/images/RNN_arc_4.png" alt="RNN-stack" width="450" />
</center>
<figcaption>
A two-stack deep RNN. The red numbers indicate the number of units in each layer. The best fitting model in the contest was a five-stack deep version of the above architecture with number of units from bottom to top of (64, 128, 256, 128, 64).
</figcaption>
</figure>

<p>I actually started out with <a href="http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf">Long-Short Term Memory</a> (LSTM) units but, in order to reduce the training time, switched to ordinary RNNs with fully connected layers, which turned out to be just as effective. My guess is that the advantages of LSTMs are only really apparent for much longer sequences than the radar sequences in this competition.</p>

<p>The final structural idea was to pass the top layer of the network, or perhaps the input layer itself, through a set of 1D convolutional and pooling layers. The motivation for this, other than the irresistable urge to throw the neural network equivalent of the kitchen sink at any and every problem, was the notion of temporal invariance—that the rain collecting in gauges should contribute the same amount to the hourly total regardless of when in the hour it actually entered the rain gauge. In my half-hearted attempt at this, my models unfortunately performed worse and I promptly abandoned it.</p>

<h3 id="nonlinearities">Nonlinearities</h3>
<p>I used ReLUs throughout with varying amounts of leakiness in the range 0.0-0.5. The goal was not to optimise this hyperparameter but to increase the variance in the final ensemble of models. Nevertheless I did find that using the original ReLU with no leakiness often resulted in very poor convergence behaviours (i.e. no improvement at all).</p>

<h2 id="training">Training</h2>

<h3 id="local-validation">Local validation</h3>
<p>I began by splitting off 20% of the training set into a stratified (with respect to the number of radar observations) validation holdout set. I soon began to distrust my setup as some models were severely overfitting on the public leaderboard despite improving local validation scores. By the end of the competition I was training my models using the entire training set and relying on the very limited number of public test submissions (two per day) to validate the models, which is exactly what one is often <strong>discouraged</strong> from doing! Due to the nature of the training and test sets in this competition (see above), I believe it was the right thing to do.</p>

<h3 id="training-procedure">Training procedure</h3>
<p>I used stochastic gradient descent (SGD) with the <a href="http://arxiv.org/abs/1212.5701">Adadelta</a> update rule with a learning rate decay.</p>

<p>I employed mini-batches of size 64 throughout; I found that the models performed consistently worse for sizes of 128 and higher, possibly because the probability of having mini-batches without any of the extreme outliers became very small. I did not get the time to properly investigate this.</p>

<p>Each model was trained over approximately 60 epochs and, depending on the size of the model, the training times ranged from one to five days per model.</p>

<h3 id="initialisation">Initialisation</h3>
<p>Most of the models used the default weight initialisation settings of the Lasagne layer classes. Towards the end of the competition I experimented with the normalized-positive definite weight matrix initialisations proposed in <a href="http://arxiv.org/abs/1511.03771">Talathi et al.</a> but found no significant effect on the performances.</p>

<h3 id="regularisation">Regularisation</h3>
<p>My biggest surprise was that implementing dropout resulted in consistently poorer results, contrary to what had been reported by many others, such as in <a href="http://arxiv.org/abs/1409.2329">Zaremba et al.</a>. I tried many combinations, including varying the dropout percentage and implementing it only at the top and/or bottom of the network, all without success. This is in stark contrast to the effectiveness of dropout when employed in the fully connected layers of CNN architectures.</p>

<p>For this reason, in the limited time available, I did not bother with weight decay as I reasoned that my main issue was under- rather than overfitting the data.</p>

<h2 id="model-ensembles">Model ensembles</h2>

<h3 id="test-time-augmentation">Test-time augmentation</h3>
<p>To predict each expected rainfall value, I took the mean of 61 separate rain gauge predictions that were generated from different <em>dropin</em> augmentations of the radar data sequence. Implementing this procedure led to a large improvement (~0.03) in the public leaderboard score.</p>

<h3 id="averaging-models">Averaging models</h3>
<p>The winning submission was a simple weighted average of 30 separate models. The best model had a public leaderboard score of 23.6971, which should be good enough for fifth place; the worst score in the winning ensemble was 23.7123. The weights were chosen on the basis of a wholly unscientific combination of public leaderboard scores and correlations between the different sets of predictions. Given the difficulties in constructing truly independent splits of the training set, there is no guarantee that the more sophisticated ensembling techniques such as stacked generalisation would be any more effective.</p>

<h2 id="final-thoughts">Final thoughts</h2>
<p>If I were to take one point away from this contest, it is that the days of manually constructing features from data are almost over. The machines will win. I experienced this in the <a href="https://www.kaggle.com/c/datasciencebowl">Plankton classification contest</a> where the monumental effort that my teammate and I put into extracting image features was eclipsed within minutes by even the shallowest of CNNs.</p>

<p>I had lots of fun in this contest and have learnt a lot. Congratulations to the other winners, and special thanks to the competition organisers and sponsors. I will make my code available soon. If you have any questions or comments, please feel free to share them.</p>

<p><strong>UPDATE (2 Jan 2016):</strong> The code is now available on GitHub: <a href="https://github.com/simaaron/kaggle-Rain">https://github.com/simaaron/kaggle-Rain</a></p>

      <footer class="entry-meta">
        <span class="entry-tags"><a href="/tags/#microbiome" title="Pages tagged microbiome" class="tag"><span class="term">microbiome</span></a><a href="/tags/#biomedical research" title="Pages tagged biomedical research" class="tag"><span class="term">biomedical research</span></a><a href="/tags/#diagnostics" title="Pages tagged diagnostics" class="tag"><span class="term">diagnostics</span></a><a href="/tags/#deep learning" title="Pages tagged deep learning" class="tag"><span class="term">deep learning</span></a></span>
        <span>Updated on <span class="entry-date date updated"><time datetime="Mon Jun 27 2016">June 27, 2016</time></span></span>
        <span class="author vcard"><span class="fn">Ali A. Faruqi</span></span>
        <div class="social-share">
  <ul class="socialcount socialcount-small inline-list">
    <li class="facebook"><a href="https://www.facebook.com/sharer/sharer.php?u=/Diagnosis-through-deep-learning-Using-microbiome-data-and-TensorFlow-A-product-for-Verily-Sciences/" title="Share on Facebook"><span class="count"><i class="fa fa-facebook-square"></i> Like</span></a></li>
    <li class="twitter"><a href="https://twitter.com/intent/tweet?text=/Diagnosis-through-deep-learning-Using-microbiome-data-and-TensorFlow-A-product-for-Verily-Sciences/" title="Share on Twitter"><span class="count"><i class="fa fa-twitter-square"></i> Tweet</span></a></li>
    <li class="googleplus"><a href="https://plus.google.com/share?url=/Diagnosis-through-deep-learning-Using-microbiome-data-and-TensorFlow-A-product-for-Verily-Sciences/" title="Share on Google Plus"><span class="count"><i class="fa fa-google-plus-square"></i> +1</span></a></li>
  </ul>
</div><!-- /.social-share -->
      </footer>
    </div><!-- /.entry-content -->
    <section id="disqus_thread"></section><!-- /#disqus_thread -->
    <div class="read-more">
  
    <div class="read-more-header">
      <a href="" class="read-more-btn">Read More</a>
    </div><!-- /.read-more-header -->
    <div class="read-more-content">
      <h3><a href="/diagnostic-platform/" title="Blog like a hacker">Blog like a hacker</a></h3>
      <p>Deep-learning based microbiome diagnostics <a href="/diagnostic-platform/">Continue reading</a></p>
    </div><!-- /.read-more-content -->
  
  <div class="read-more-list">
    
  </div><!-- /.read-more-list -->
</div><!-- /.read-more -->
  </article>
</div><!-- /#main -->

<div class="footer-wrapper">
  <footer role="contentinfo">
    <span>&copy; 2016 Ali A. Faruqi. Powered by <a href="http://jekyllrb.com" rel="nofollow">Jekyll</a> using the <a href="https://mademistakes.com/work/hpstr-jekyll-theme/" rel="nofollow">HPSTR Theme</a>.</span>
  </footer>
</div><!-- /.footer-wrapper -->

<script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
<script>window.jQuery || document.write('<script src="/assets/js/vendor/jquery-1.9.1.min.js"><\/script>')</script>
<script src="/assets/js/scripts.min.js"></script>


<!-- Asynchronous Google Analytics snippet -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-71209807-1', 'auto');  
  ga('require', 'linkid', 'linkid.js');
  ga('send', 'pageview');
</script>



    <script type="text/javascript">
        /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
        var disqus_shortname = 'alifar76'; // required: replace example with your forum shortname

        /* * * DON'T EDIT BELOW THIS LINE * * */
        (function() {
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();

        /* * * DON'T EDIT BELOW THIS LINE * * */
        (function () {
            var s = document.createElement('script'); s.async = true;
            s.type = 'text/javascript';
            s.src = '//' + disqus_shortname + '.disqus.com/count.js';
            (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
        }());
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
	        

</body>
</html>
